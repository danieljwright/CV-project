{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-09T09:26:32.654239Z","iopub.status.busy":"2024-06-09T09:26:32.653829Z","iopub.status.idle":"2024-06-09T09:26:33.535157Z","shell.execute_reply":"2024-06-09T09:26:33.534121Z","shell.execute_reply.started":"2024-06-09T09:26:32.654202Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:26:33.545286Z","iopub.status.busy":"2024-06-09T09:26:33.544949Z","iopub.status.idle":"2024-06-09T09:26:42.422038Z","shell.execute_reply":"2024-06-09T09:26:42.420798Z","shell.execute_reply.started":"2024-06-09T09:26:33.545253Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchvision\n","from torchvision import datasets, transforms\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","import copy\n","from tqdm import tqdm\n","\n","\n","# Define a transform to normalize the data\n","transform = transforms.Compose([\n","    transforms.ToTensor(),  # Convert images to tensors\n","    transforms.Normalize((0.5,), (0.5,))  # Normalize the pixel values to range [-1, 1]\n","])\n","\n","# Download the training data\n","trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n","# Download the testset\n","testset = datasets.MNIST('~/.pytorch/MNIST_data', train=False,\n","                                       download=True, transform=transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:26:42.424148Z","iopub.status.busy":"2024-06-09T09:26:42.423419Z","iopub.status.idle":"2024-06-09T09:26:42.435406Z","shell.execute_reply":"2024-06-09T09:26:42.434395Z","shell.execute_reply.started":"2024-06-09T09:26:42.424105Z"},"trusted":true},"outputs":[],"source":["indices = list(range(len(trainset)))\n","np.random.shuffle(indices)\n","shuffled_trainset = torch.utils.data.Subset(trainset, indices)\n","print(len(shuffled_trainset))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:26:42.437422Z","iopub.status.busy":"2024-06-09T09:26:42.436416Z","iopub.status.idle":"2024-06-09T09:26:42.448866Z","shell.execute_reply":"2024-06-09T09:26:42.447959Z","shell.execute_reply.started":"2024-06-09T09:26:42.437395Z"},"trusted":true},"outputs":[],"source":["'''dataset1 = []\n","dataset2 = []\n","dataset3 = []\n","dataset4 = []\n","dataset5 = []\n","dataset6 = []\n","\n","for i in range(10000):\n","    dataset1 = torch.utils.data.Subset(full_dataset, list(range(10000)))\n","    dataset1.append(shuffled_trainset[i])\n","    \n","for i in range(10000, 20000):\n","    \n","    dataset2.append(shuffled_trainset[i])\n","    \n","for i in range(20000, 30000):\n","    \n","    dataset3.append(shuffled_trainset[i])\n","    \n","for i in range(30000, 40000):\n","    \n","    dataset4.append(shuffled_trainset[i])\n","    \n","for i in range(40000, 50000):\n","    \n","    dataset5.append(shuffled_trainset[i])\n","for i in range(50000, 60000):\n","    \n","    dataset6.append(shuffled_trainset[i])'''\n","    \n","dataset1 = torch.utils.data.Subset(shuffled_trainset, list(range(10000)))\n","dataset2 = torch.utils.data.Subset(shuffled_trainset, list(range(10000, 20000)))\n","dataset3 = torch.utils.data.Subset(shuffled_trainset, list(range(20000, 30000)))\n","dataset4 = torch.utils.data.Subset(shuffled_trainset, list(range(30000, 40000)))\n","dataset5 = torch.utils.data.Subset(shuffled_trainset, list(range(40000, 50000)))\n","dataset6 = torch.utils.data.Subset(shuffled_trainset, list(range(50000, 60000)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:26:42.450291Z","iopub.status.busy":"2024-06-09T09:26:42.450007Z","iopub.status.idle":"2024-06-09T09:26:42.461114Z","shell.execute_reply":"2024-06-09T09:26:42.460268Z","shell.execute_reply.started":"2024-06-09T09:26:42.450266Z"},"trusted":true},"outputs":[],"source":["batch_size=64\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False)\n","trainloader = torch.utils.data.DataLoader(shuffled_trainset, batch_size=batch_size,\n","                                         shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:26:42.462805Z","iopub.status.busy":"2024-06-09T09:26:42.462417Z","iopub.status.idle":"2024-06-09T09:26:42.470759Z","shell.execute_reply":"2024-06-09T09:26:42.469873Z","shell.execute_reply.started":"2024-06-09T09:26:42.462768Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(npimg)\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:26:42.472722Z","iopub.status.busy":"2024-06-09T09:26:42.472032Z","iopub.status.idle":"2024-06-09T09:26:42.513841Z","shell.execute_reply":"2024-06-09T09:26:42.512893Z","shell.execute_reply.started":"2024-06-09T09:26:42.472686Z"},"trusted":true},"outputs":[],"source":["from torchvision.transforms import v2\n","\n","def apply_perspective_transform(image):\n","\n","\n","    mean = torch.mean(image)\n","    std = torch.std(image)\n","\n","    perspective_transform = v2.Compose([\n","        v2.RandomPerspective(distortion_scale=0.3, p=1.0, fill=-1),\n","        v2.ToDtype(torch.float32, scale=True),\n","        #v2.Normalize(mean=[mean], std=[std]),\n","    ])\n","\n","    transformed_image = perspective_transform(image)\n","    transformed_image = transformed_image.reshape(28,28)\n","\n","    return transformed_image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:26:42.519266Z","iopub.status.busy":"2024-06-09T09:26:42.518961Z","iopub.status.idle":"2024-06-09T09:26:42.904421Z","shell.execute_reply":"2024-06-09T09:26:42.903402Z","shell.execute_reply.started":"2024-06-09T09:26:42.519241Z"},"trusted":true},"outputs":[],"source":["out = apply_perspective_transform(dataset1[0][0])\n","imshow(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:26:42.905788Z","iopub.status.busy":"2024-06-09T09:26:42.905507Z","iopub.status.idle":"2024-06-09T09:26:43.127753Z","shell.execute_reply":"2024-06-09T09:26:43.126498Z","shell.execute_reply.started":"2024-06-09T09:26:42.905763Z"},"trusted":true},"outputs":[],"source":["def apply_blur(image):\n","\n","\n","    blurrer = v2.GaussianBlur(kernel_size=5, sigma=(1, 2.))\n","\n","    transformed_image = blurrer(image)\n","    transformed_image = transformed_image.reshape(28,28)\n","\n","    return transformed_image\n","\n","\n","out = apply_blur(dataset1[0][0])\n","\n","\n","imshow(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:26:43.129277Z","iopub.status.busy":"2024-06-09T09:26:43.128967Z","iopub.status.idle":"2024-06-09T09:26:43.368979Z","shell.execute_reply":"2024-06-09T09:26:43.368053Z","shell.execute_reply.started":"2024-06-09T09:26:43.129250Z"},"trusted":true},"outputs":[],"source":["def apply_gaussian_noise(image):\n","\n","    noise = np.random.normal(scale=0.5,size=(28,28))\n","\n","    transformed_image = image + noise\n","    transformed_image = transformed_image.reshape(28,28)\n","\n","    return transformed_image\n","\n","out = apply_gaussian_noise(dataset1[0][0])\n","\n","\n","imshow(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:26:43.370676Z","iopub.status.busy":"2024-06-09T09:26:43.370313Z","iopub.status.idle":"2024-06-09T09:26:43.606832Z","shell.execute_reply":"2024-06-09T09:26:43.605922Z","shell.execute_reply.started":"2024-06-09T09:26:43.370642Z"},"trusted":true},"outputs":[],"source":["def apply_random_erase(image):\n","\n","\n","    erase = v2.RandomErasing(value=-1)\n","\n","    transformed_image = erase(image)\n","    transformed_image = transformed_image.reshape(28,28)\n","\n","    return transformed_image\n","\n","out = apply_random_erase(dataset1[0][0])\n","\n","\n","imshow(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:26:43.608458Z","iopub.status.busy":"2024-06-09T09:26:43.608087Z","iopub.status.idle":"2024-06-09T09:26:43.848065Z","shell.execute_reply":"2024-06-09T09:26:43.847181Z","shell.execute_reply.started":"2024-06-09T09:26:43.608424Z"},"trusted":true},"outputs":[],"source":["def apply_elastic_transform(image):\n","    elastic_transformer = v2.ElasticTransform(alpha=50.0)\n","    transformed_image = elastic_transformer(image)\n","    transformed_image = transformed_image.reshape(28, 28)\n","    return transformed_image\n","\n","out = apply_elastic_transform(dataset1[0][0])\n","imshow(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:26:43.849768Z","iopub.status.busy":"2024-06-09T09:26:43.849378Z","iopub.status.idle":"2024-06-09T09:26:44.087833Z","shell.execute_reply":"2024-06-09T09:26:44.086944Z","shell.execute_reply.started":"2024-06-09T09:26:43.849730Z"},"trusted":true},"outputs":[],"source":["def apply_random_rotation(image):\n","    rotater = v2.RandomRotation(degrees=(0, 30))\n","    transformed_image = rotater(image)\n","    transformed_image = transformed_image.reshape(28, 28)\n","    return transformed_image\n","\n","out = apply_random_rotation(dataset1[0][0])\n","imshow(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:26:44.089460Z","iopub.status.busy":"2024-06-09T09:26:44.089070Z","iopub.status.idle":"2024-06-09T09:27:53.046867Z","shell.execute_reply":"2024-06-09T09:27:53.045838Z","shell.execute_reply.started":"2024-06-09T09:26:44.089410Z"},"trusted":true},"outputs":[],"source":["'''print(len(dataset1))\n","train_1 = []\n","for i in range(10000):\n","    train_1.append([apply_perspective_transform(dataset1[i][0]), dataset1[i][1]])\n","print(len(train_1))\n","train_2 = []\n","for i in range(10000):\n","    train_2.append(apply_blur(dataset2[i][0]))\n","train_3 = []\n","for i in range(10000):\n","    train_3.append(apply_gaussian_noise(dataset3[i][0]))\n","train_4 = []\n","for i in range(10000):\n","    train_4.append(apply_random_erase(dataset4[i][0]))\n","train_5 = []\n","for i in range(10000):\n","    train_5.append(apply_elastic_transform(dataset5[i][0]))\n","train_6 = []\n","for i in range(10000):\n","    train_6.append(apply_random_rotation(dataset6[i][0]))'''\n","\n","for i in range(10000):\n","    dataset1[i][0] == apply_perspective_transform(dataset1[i][0])\n","for i in range(10000):\n","    dataset2[i][0] == apply_blur(dataset2[i][0])\n","for i in range(10000):\n","    dataset3[i][0] == apply_gaussian_noise(dataset3[i][0])\n","for i in range(10000):\n","    dataset4[i][0] == apply_random_erase(dataset4[i][0])\n","for i in range(10000):\n","    dataset5[i][0] == apply_elastic_transform(dataset5[i][0])\n","for i in range(10000):\n","    dataset6[i][0] == apply_random_rotation(dataset6[i][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:27:53.049707Z","iopub.status.busy":"2024-06-09T09:27:53.049263Z","iopub.status.idle":"2024-06-09T09:27:53.058774Z","shell.execute_reply":"2024-06-09T09:27:53.057643Z","shell.execute_reply.started":"2024-06-09T09:27:53.049670Z"},"trusted":true},"outputs":[],"source":["batch_size=64\n","trainloaders = []\n","train_loader_1 = torch.utils.data.DataLoader(dataset1, batch_size=batch_size, shuffle=False)\n","train_loader_2 = torch.utils.data.DataLoader(dataset2, batch_size=batch_size, shuffle=False)\n","train_loader_3 = torch.utils.data.DataLoader(dataset3, batch_size=batch_size, shuffle=False)\n","train_loader_4 = torch.utils.data.DataLoader(dataset4, batch_size=batch_size, shuffle=False)\n","train_loader_5 = torch.utils.data.DataLoader(dataset5, batch_size=batch_size, shuffle=False)\n","train_loader_6 = torch.utils.data.DataLoader(dataset6, batch_size=batch_size, shuffle=False)\n","trainloaders.append(trainloader)\n","trainloaders.append(train_loader_1)\n","trainloaders.append(train_loader_2)\n","trainloaders.append(train_loader_3)\n","trainloaders.append(train_loader_4)\n","trainloaders.append(train_loader_5)\n","trainloaders.append(train_loader_6)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:27:53.060592Z","iopub.status.busy":"2024-06-09T09:27:53.060272Z","iopub.status.idle":"2024-06-09T09:27:53.077255Z","shell.execute_reply":"2024-06-09T09:27:53.076368Z","shell.execute_reply.started":"2024-06-09T09:27:53.060562Z"},"trusted":true},"outputs":[],"source":["class CustomCNN(nn.Module):\n","    def __init__(self):\n","        super(CustomCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(2, 4, kernel_size=3, stride=1, padding=1)\n","        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","        self.fc1 = nn.Linear(4 * 7 * 7, 16)\n","        self.fc2 = nn.Linear(16, 10)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.max_pool(self.relu(self.conv1(x)))\n","        x = self.max_pool(self.relu(self.conv2(x)))\n","        x = x.view(-1, 4 * 7 * 7)\n","        x = self.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","    \n","\n","def generate_models():\n","\n","    models = []\n","    rand_model = CustomCNN()\n","    model1 = CustomCNN()\n","    model2 = CustomCNN()\n","    model3 = CustomCNN()\n","    model4 = CustomCNN()\n","    model5 = CustomCNN()\n","    model6 = CustomCNN()\n","\n","    \n","    models.append(model1)\n","    models.append(model2)\n","    models.append(model3)\n","    models.append(model4)\n","    models.append(model5)\n","    models.append(model6)\n","\n","    initial_rand_parms = copy.deepcopy(rand_model.state_dict())\n","\n","    return rand_model, models, initial_rand_parms"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:27:53.078514Z","iopub.status.busy":"2024-06-09T09:27:53.078253Z","iopub.status.idle":"2024-06-09T09:27:53.087865Z","shell.execute_reply":"2024-06-09T09:27:53.087178Z","shell.execute_reply.started":"2024-06-09T09:27:53.078492Z"},"trusted":true},"outputs":[],"source":["def gen_optimizers(rand_model, models):\n","    \n","        optimizers = []\n","        rand_optimizer = optim.Adam(rand_model.parameters(), lr=0.001)\n","        optimizer1 = optim.Adam(models[0].parameters(), lr=0.001)\n","        optimizer2 = optim.Adam(models[1].parameters(), lr=0.001)\n","        optimizer3 = optim.Adam(models[2].parameters(), lr=0.001)\n","        optimizer4 = optim.Adam(models[3].parameters(), lr=0.001)\n","        optimizer5 = optim.Adam(models[4].parameters(), lr=0.001)\n","        optimizer6 = optim.Adam(models[5].parameters(), lr=0.001)\n","\n","        optimizers.append(optimizer1)\n","        optimizers.append(optimizer2)\n","        optimizers.append(optimizer3)\n","        optimizers.append(optimizer4)\n","        optimizers.append(optimizer5)\n","        optimizers.append(optimizer6)\n","    \n","        return rand_optimizer, optimizers\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","# optimizers = []\n","# optimizer = optim.Adam(model.parameters(), lr=0.001)\n","# optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n","# optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n","# optimizer3 = optim.Adam(model3.parameters(), lr=0.001)\n","# optimizer4 = optim.Adam(model4.parameters(), lr=0.001)\n","# optimizer5 = optim.Adam(model5.parameters(), lr=0.001)\n","# optimizer6 = optim.Adam(model6.parameters(), lr=0.001)\n","# optimizers.append(optimizer)\n","# optimizers.append(optimizer1)\n","# optimizers.append(optimizer2)\n","# optimizers.append(optimizer3)\n","# optimizers.append(optimizer4)\n","# optimizers.append(optimizer5)\n","# optimizers.append(optimizer6)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:27:53.089579Z","iopub.status.busy":"2024-06-09T09:27:53.089061Z","iopub.status.idle":"2024-06-09T09:28:26.418284Z","shell.execute_reply":"2024-06-09T09:28:26.417351Z","shell.execute_reply.started":"2024-06-09T09:27:53.089544Z"},"trusted":true},"outputs":[],"source":["def train_rand_model(rand_model, rand_optimizer):\n","\n","    num_epochs = 2\n","\n","    initial_params = copy.deepcopy(rand_model.state_dict())\n","    \n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        for i, data in enumerate(trainloader, 0):\n","            inputs, labels = data\n","            \n","            rand_optimizer.zero_grad()\n","            \n","            outputs = rand_model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            rand_optimizer.step()\n","            \n","            running_loss += loss.item()\n","            if i % 100 == 99:  # print every 200 mini-batches\n","                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(trainloader)}], Loss: {running_loss / 200:.4f}')\n","                running_loss = 0.0\n","    \n","    #print('Finished Training')\n","\n","    final_params = rand_model.state_dict()\n","\n","    return initial_params, final_params\n","\n","\n","\n","\n","def train_models(models, optimizers):\n","\n","    num_epochs = 6\n","    \n","    criterion = nn.CrossEntropyLoss()\n","    for j in tqdm(range(6)):\n","        for epoch in range(num_epochs):\n","            running_loss = 0.0\n","            for i, data in enumerate(trainloaders[j], 0):\n","                #print(len(data))\n","                inputs, labels = data\n","                \n","                optimizers[j].zero_grad()\n","                \n","                outputs = models[j](inputs)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizers[j].step()\n","                \n","                running_loss += loss.item()\n","                if i % 100 == 99:  # print every 200 mini-batches\n","                    print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(trainloaders[j])}], Loss: {running_loss / 200:.4f}')\n","                    running_loss = 0.0\n","        \n","    #print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:28:26.420363Z","iopub.status.busy":"2024-06-09T09:28:26.419624Z","iopub.status.idle":"2024-06-09T09:28:28.750717Z","shell.execute_reply":"2024-06-09T09:28:28.749863Z","shell.execute_reply.started":"2024-06-09T09:28:26.420331Z"},"trusted":true},"outputs":[],"source":["def test_model(model):\n","\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    #print(f'Accuracy of th randomly initialized network on the 10000 test images: {100 * correct / total:.2f}%')\n","    return 100 * correct / total\n","\n","\n","# correct = 0\n","# total = 0\n","\n","# with torch.no_grad():\n","#     for data in testloader:\n","#         images, labels = data\n","#         outputs = model(images)\n","#         _, predicted = torch.max(outputs.data, 1)\n","#         total += labels.size(0)\n","#         correct += (predicted == labels).sum().item()\n","\n","# print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:28:28.752069Z","iopub.status.busy":"2024-06-09T09:28:28.751790Z","iopub.status.idle":"2024-06-09T09:28:28.765638Z","shell.execute_reply":"2024-06-09T09:28:28.764620Z","shell.execute_reply.started":"2024-06-09T09:28:28.752043Z"},"trusted":true},"outputs":[],"source":["# state_dict = model.state_dict()\n","# for param_tensor in state_dict:\n","#     print(f\"{param_tensor}:\\n {state_dict[param_tensor].numpy()}\\n\")\n","    \n","# filters = state_dict[\"conv2.weight\"]\n","# extracted = []\n","# for i in range(filters.shape[0]):\n","#     for j in range(filters.shape[1]):\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:28:28.767724Z","iopub.status.busy":"2024-06-09T09:28:28.766949Z","iopub.status.idle":"2024-06-09T09:28:28.774449Z","shell.execute_reply":"2024-06-09T09:28:28.773463Z","shell.execute_reply.started":"2024-06-09T09:28:28.767684Z"},"trusted":true},"outputs":[],"source":["'''from torch.nn import functional as F\n","\n","# calculate similarity along the rows\n","print(extracted[0])\n","tensor1 = extracted[0].unsqueeze(1)\n","tensor2 = extracted[1].unsqueeze(1)\n","cosine_similarity_row = F.cosine_similarity(tensor1, tensor2, dim=2)\n","print(cosine_similarity_row)'''\n","\n","'''# PAIRWISE SIMILARITY\n","from sklearn.metrics.pairwise import pairwise_distances\n","from torch.nn import functional as F\n","\n","X = extracted[0]\n","Y = extracted[1]\n","\n","sim = pairwise_distances(X, Y, metric='cosine')\n","print(sim)\n","\n","\n","Xf = extracted[0].view(-1)\n","Yf = extracted[1].view(-1)\n","\n","simF = F.cosine_similarity(Xf, Yf, dim=0)\n","print(simF)'''\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:36:39.272519Z","iopub.status.busy":"2024-06-09T09:36:39.271777Z","iopub.status.idle":"2024-06-09T09:36:39.277993Z","shell.execute_reply":"2024-06-09T09:36:39.276932Z","shell.execute_reply.started":"2024-06-09T09:36:39.272482Z"},"trusted":true},"outputs":[],"source":["def calc_overall_sim(tensor1, tensor2):\n","    \n","    tensor1_flat = tensor1.view(-1)\n","    tensor2_flat = tensor2.view(-1)\n","    \n","    similarity = F.cosine_similarity(tensor1_flat, tensor2_flat, dim=0)\n","    similarity = torch.abs(similarity)\n","    \n","    return similarity.detach().numpy().reshape(1,)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:36:41.152700Z","iopub.status.busy":"2024-06-09T09:36:41.151817Z","iopub.status.idle":"2024-06-09T09:36:41.157518Z","shell.execute_reply":"2024-06-09T09:36:41.156477Z","shell.execute_reply.started":"2024-06-09T09:36:41.152664Z"},"trusted":true},"outputs":[],"source":["def calc_pearson_coeff(tensor1, tensor2):\n","    corr_coeff = np.corrcoef(tensor1.detach().numpy().flatten(), tensor2.detach().numpy().flatten())[0,1]\n","    corr_coeff = np.abs(corr_coeff)\n","    return corr_coeff"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:36:41.618615Z","iopub.status.busy":"2024-06-09T09:36:41.618228Z","iopub.status.idle":"2024-06-09T09:36:41.624416Z","shell.execute_reply":"2024-06-09T09:36:41.623365Z","shell.execute_reply.started":"2024-06-09T09:36:41.618582Z"},"trusted":true},"outputs":[],"source":["def calc_frob_norm(tensor1, tensor2):\n","    A = tensor1.detach().numpy()\n","    B = tensor2.detach().numpy()\n","    frobenius_norm = np.linalg.norm(A - B, ord='fro')\n","    return frobenius_norm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:36:42.228054Z","iopub.status.busy":"2024-06-09T09:36:42.227017Z","iopub.status.idle":"2024-06-09T09:36:42.270644Z","shell.execute_reply":"2024-06-09T09:36:42.269747Z","shell.execute_reply.started":"2024-06-09T09:36:42.228020Z"},"trusted":true},"outputs":[],"source":["from torch.nn import functional as F\n","import matplotlib.pyplot as plt\n","#similarity_scores = np.zeros((6,8))\n","def compare_filters(models, metrics=['overall_sim', 'pearson_coeff', 'frob_norm']):\n","\n","    similarity_scores_1 = []\n","    similarity_scores_2 = []\n","    similarity_scores_3 = []\n","    similarity_scores_4 = []\n","    similarity_scores_5 = []\n","    similarity_scores_6 = []\n","    similarity_scores_7 = []\n","    similarity_scores_8 = []\n","    similarity_scores_9 = []\n","    similarity_scores_10 = []\n","\n","    for i in range(0, 6):\n","\n","        cnn_filters_i = models[i].conv1.weight\n","        cnn_filters_i = cnn_filters_i.view(cnn_filters_i.size(0)*cnn_filters_i.size(1), cnn_filters_i.size(2), cnn_filters_i.size(3))   \n","        for j in range(2):\n","\n","            filter1 = cnn_filters_i[j]\n","\n","            for k in range(i+1, 6):\n","                cnn_filters_k = models[k].conv1.weight\n","                cnn_filters_k = cnn_filters_k.view(cnn_filters_k.size(0)*cnn_filters_k.size(1), cnn_filters_k.size(2), cnn_filters_k.size(3))\n","                filter2 = cnn_filters_k[j]\n","                if metrics == 'overall_sim':\n","                    similarity = calc_overall_sim(filter1, filter2)\n","                elif metrics == 'pearson_coeff': \n","                    similarity = calc_pearson_coeff(filter1, filter2)\n","                elif metrics == 'frob_norm':\n","                    similarity = calc_frob_norm(filter1, filter2)\n","                #similarity = calc_overall_sim(filter1, filter2)\n","                #similarity = calc_pearson_coeff(filter1, filter2)\n","                #similarity = calc_frob_norm(filter1, filter2)\n","\n","                if j==0:\n","                    similarity_scores_1.append(similarity)\n","\n","                if j==1:\n","                    similarity_scores_2.append(similarity)\n","            \n","\n","\n","        cnn_filters = models[i].conv2.weight\n","        cnn_filters = cnn_filters.view(cnn_filters.size(0)*cnn_filters.size(1), cnn_filters.size(2), cnn_filters.size(3))\n","        print(cnn_filters.shape)\n","        \n","        for j in range(8):\n","            filter1 = cnn_filters[j]\n","            \n","            for k in range(i+1, 6):  # Fix: adjust the range to avoid out-of-bounds indices\n","                cnn_filters2 = models[k].conv2.weight\n","                cnn_filters2 = cnn_filters2.view(cnn_filters2.size(0)*cnn_filters2.size(1), cnn_filters2.size(2), cnn_filters2.size(3))\n","                filter2 = cnn_filters2[j]\n","                \n","                if metrics == 'overall_sim':\n","                    similarity = calc_overall_sim(filter1, filter2)\n","                elif metrics == 'pearson_coeff':\n","                    similarity = calc_pearson_coeff(filter1, filter2)\n","                elif metrics == 'frob_norm':\n","                    similarity = calc_frob_norm(filter1, filter2)\n","                #similarity = calc_overall_sim(filter1, filter2)\n","                #similarity = calc_pearson_coeff(filter1, filter2)\n","                #similarity = calc_frob_norm(filter1, filter2)\n","                \n","                if j==0:\n","                    similarity_scores_3.append(similarity)\n","                \n","                if j==1:\n","                    similarity_scores_4.append(similarity)\n","                \n","                if j==2:\n","                    similarity_scores_5.append(similarity)\n","                \n","                if j==3:\n","                    similarity_scores_6.append(similarity)\n","                \n","                if j==4:\n","                    similarity_scores_7.append(similarity)\n","                \n","                if j==5:\n","                    similarity_scores_8.append(similarity)\n","                \n","                if j==6:\n","                    similarity_scores_9.append(similarity)\n","                    \n","                if j==7:\n","                    similarity_scores_10.append(similarity)  \n","\n","\n","\n","\n","\n","\n","\n","    # print(similarity_scores_1)\n","    # print(similarity_scores_2)\n","    # print(similarity_scores_3)\n","    # print(similarity_scores_4)\n","    # print(similarity_scores_5)\n","    # print(similarity_scores_6)\n","    # print(similarity_scores_7)\n","    # print(similarity_scores_8)\n","    # print(similarity_scores_9)\n","    # print(similarity_scores_10)\n","\n","    return similarity_scores_1, similarity_scores_2, similarity_scores_3, similarity_scores_4, similarity_scores_5, similarity_scores_6, similarity_scores_7, similarity_scores_8, similarity_scores_9, similarity_scores_10\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T09:28:28.842889Z","iopub.status.busy":"2024-06-09T09:28:28.842588Z","iopub.status.idle":"2024-06-09T09:28:28.855010Z","shell.execute_reply":"2024-06-09T09:28:28.853990Z","shell.execute_reply.started":"2024-06-09T09:28:28.842864Z"},"trusted":true},"outputs":[],"source":["\n","\n","# filters_1 = []\n","# filters_2 = []\n","# filters_3 = []\n","# filters_4 = []\n","# filters_5 = []\n","# filters_6 = []\n","# filters_7 = []\n","# filters_8 = []\n","# filters_9 = []\n","# filters_10 = []\n","# for i in range(1,7):\n","#     cnn_filters = models[i].conv1.weight\n","#     cnn_filters = cnn_filters.view(cnn_filters.size(0)*cnn_filters.size(1), cnn_filters.size(2), cnn_filters.size(3))\n","#     for j in range(2):\n","#         if j==0:\n","#             filters_1.append(cnn_filters[j].detach().numpy())\n","#         if j==1:\n","#             filters_2.append(cnn_filters[j].detach().numpy())\n","#     cnn_filters = models[i].conv2.weight\n","#     cnn_filters = cnn_filters.view(cnn_filters.size(0)*cnn_filters.size(1), cnn_filters.size(2), cnn_filters.size(3))\n","\n","#     for j in range(8):\n","#         if j==0:\n","#             filters_3.append(cnn_filters[j].detach().numpy())\n","#         if j==1:\n","#             filters_4.append(cnn_filters[j].detach().numpy())\n","#         if j==2:\n","#             filters_5.append(cnn_filters[j].detach().numpy())\n","#         if j==3:\n","#             filters_6.append(cnn_filters[j].detach().numpy())\n","#         if j==4:\n","#             filters_7.append(cnn_filters[j].detach().numpy())\n","#         if j==5:\n","#             filters_8.append(cnn_filters[j].detach().numpy())\n","#         if j==6:\n","#             filters_9.append(cnn_filters[j].detach().numpy())  \n","#         if j==7:   \n","#             filters_10.append(cnn_filters[j].detach().numpy())\n","            \n","# matrices = np.array(filters_1)\n","# variance_matrix = np.var(matrices, axis=0)   \n","# print(variance_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def get_max_index(similarity_scores):\n","    max_index = np.argmax(similarity_scores)\n","    return max_index\n","\n","def get_max_index(similarity_scores_1, similarity_scores_2, similarity_scores_3, similarity_scores_4, similarity_scores_5, similarity_scores_6, similarity_scores_7, similarity_scores_8, similarity_scores_9, similarity_scores_10):\n","\n","    filter1_idx = np.argmax(similarity_scores_1)\n","    filter2_idx = np.argmax(similarity_scores_2)\n","    filter3_idx = np.argmax(similarity_scores_3)\n","    filter4_idx = np.argmax(similarity_scores_4)\n","    filter5_idx = np.argmax(similarity_scores_5)\n","    filter6_idx = np.argmax(similarity_scores_6)\n","    filter7_idx = np.argmax(similarity_scores_7)\n","    filter8_idx = np.argmax(similarity_scores_8)\n","    filter9_idx = np.argmax(similarity_scores_9)\n","    filter10_idx = np.argmax(similarity_scores_10)\n","\n","    indexes = [filter1_idx, filter2_idx, filter3_idx, filter4_idx, filter5_idx, filter6_idx, filter7_idx, filter8_idx, filter9_idx, filter10_idx]\n","\n","\n","    # print(\"Filter 1 index:\", filter1_idx)\n","    # print(\"Filter 2 index:\", filter2_idx)\n","    # print(\"Filter 3 index:\", filter3_idx)\n","    # print(\"Filter 4 index:\", filter4_idx)\n","    # print(\"Filter 5 index:\", filter5_idx)\n","    # print(\"Filter 6 index:\", filter6_idx)\n","    # print(\"Filter 7 index:\", filter7_idx)\n","    # print(\"Filter 8 index:\", filter8_idx)\n","    # print(\"Filter 9 index:\", filter9_idx)\n","    # print(\"Filter 10 index:\", filter10_idx)\n","\n","    return indexes\n","\n","def get_min_index(similarity_scores_1, similarity_scores_2, similarity_scores_3, similarity_scores_4, similarity_scores_5, similarity_scores_6, similarity_scores_7, similarity_scores_8, similarity_scores_9, similarity_scores_10):\n","\n","    filter1_idx = np.argmin(similarity_scores_1)\n","    filter2_idx = np.argmin(similarity_scores_2)\n","    filter3_idx = np.argmin(similarity_scores_3)\n","    filter4_idx = np.argmin(similarity_scores_4)\n","    filter5_idx = np.argmin(similarity_scores_5)\n","    filter6_idx = np.argmin(similarity_scores_6)\n","    filter7_idx = np.argmin(similarity_scores_7)\n","    filter8_idx = np.argmin(similarity_scores_8)\n","    filter9_idx = np.argmin(similarity_scores_9)\n","    filter10_idx = np.argmin(similarity_scores_10)\n","\n","    indexes = [filter1_idx, filter2_idx, filter3_idx, filter4_idx, filter5_idx, filter6_idx, filter7_idx, filter8_idx, filter9_idx, filter10_idx]\n","\n","    return indexes\n","\n","\n","\n","\n","\n","#(1,2) (1,3) (1,4) (1,5) (1,6) (2,3) (2,4) (2,5) (2,6)  (3,4) (3,5) (3,6) (4,5) (4,6) (5,6)\n","\n","\n","#with torch.no_grad():\n"," #   net.conv1.weight[1][1] = torch.zeros((3, 3))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Hand coding in the filters\n","\n","def select_filters(models, indexes):\n","    selected_filters = []\n","\n","    for i, idx in enumerate(indexes):\n","        if idx in range(0,5):\n","            if i == 0 or i == 1:\n","                selected_filters.append(models[1].conv1.weight[i])\n","            else:\n","                selected_filters.append(models[1].conv2.weight[(i//2)-1][i%2])\n","\n","        if idx in range(5,9):\n","            if i == 0 or i == 1:\n","                selected_filters.append(models[2].conv1.weight[i])\n","            else:\n","                selected_filters.append(models[2].conv2.weight[(i//2)-1][i%2])\n","        if idx in range(9,12):\n","            if i == 0 or i == 1:\n","                selected_filters.append(models[3].conv1.weight[i])\n","            else:\n","                selected_filters.append(models[3].conv2.weight[(i//2)-1][i%2])\n","        if idx in range(12,14):\n","            if i == 0 or i == 1:\n","                selected_filters.append(models[4].conv1.weight[i])\n","            else:\n","                selected_filters.append(models[4].conv2.weight[(i//2)-1][i%2])\n","        if idx in range(14,15):\n","            if i == 0 or i == 1:\n","                selected_filters.append(models[5].conv1.weight[i])\n","            else:\n","                selected_filters.append(models[5].conv2.weight[(i//2)-1][i%2])\n","\n","    return selected_filters\n","\n","# selected_filters = []\n","\n","# for i, idx in enumerate(indexes):\n","#     if idx in range(0,5):\n","#         if i == 0 or i == 1:\n","#             selected_filters.append(model1.conv1.weight[i])\n","#         else:\n","#             selected_filters.append(model1.conv2.weight[(i//2)-1][i%2])\n","\n","#     if idx in range(5,9):\n","#         if i == 0 or i == 1:\n","#             selected_filters.append(model2.conv1.weight[i])\n","#         else:\n","#             selected_filters.append(model2.conv2.weight[(i//2)-1][i%2])\n","#     if idx in range(9,12):\n","#         if i == 0 or i == 1:\n","#             selected_filters.append(model3.conv1.weight[i])\n","#         else:\n","#             selected_filters.append(model3.conv2.weight[(i//2)-1][i%2])\n","#     if idx in range(12,14):\n","#         if i == 0 or i == 1:\n","#             selected_filters.append(model4.conv1.weight[i])\n","#         else:\n","#             selected_filters.append(model4.conv2.weight[(i//2)-1][i%2])\n","#     if idx in range(14,15):\n","#         if i == 0 or i == 1:\n","#             selected_filters.append(model5.conv1.weight[i])\n","#         else:\n","#             selected_filters.append(model5.conv2.weight[(i//2)-1][i%2])\n","    \n","        \n","        \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Warm starting a model with the selected filters\n","from torchsummary import summary\n","\n","\n","def gen_warm_start_model():\n","\n","    model_warm_start = CustomCNN()\n","    \n","\n","    with torch.no_grad():\n","        model_warm_start.conv1.weight[0] = selected_filters[0]\n","        model_warm_start.conv1.weight[1] = selected_filters[1]\n","        model_warm_start.conv2.weight[0][0] = selected_filters[2]\n","        model_warm_start.conv2.weight[0][1] = selected_filters[3]\n","        model_warm_start.conv2.weight[1][0] = selected_filters[4]\n","        model_warm_start.conv2.weight[1][1] = selected_filters[5]\n","        model_warm_start.conv2.weight[2][0] = selected_filters[6]\n","        model_warm_start.conv2.weight[2][1] = selected_filters[7]\n","        model_warm_start.conv2.weight[3][0] = selected_filters[8]\n","        model_warm_start.conv2.weight[3][1] = selected_filters[9]\n","\n","    model_warm_start.requires_grad = True\n","    torch.enable_grad\n","\n","    return model_warm_start\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def train_warm_start_model(model_warm_start):\n","    num_epochs = 1\n","\n","    optimizer_warm= optim.Adam(model_warm_start.parameters(), lr=0.001)\n","\n","\n","    initial_params = copy.deepcopy(model_warm_start.state_dict())\n","    #print(initial_params.keys())\n","\n","    with torch.enable_grad():\n","\n","        for epoch in range(num_epochs):\n","            running_loss = 0.0\n","            for i, data in enumerate(trainloaders[0], 0):\n","                #print(len(data))\n","                inputs, labels = data\n","                \n","                optimizer_warm.zero_grad()\n","                \n","                outputs = model_warm_start(inputs)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer_warm.step()\n","                \n","                running_loss += loss.item()\n","                if i % 100 == 99:  # print every 200 mini-batches\n","                    print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(trainloaders[0])}], Loss: {running_loss / 200:.4f}')\n","                    running_loss = 0.0\n","        \n","    # print('Finished Training')\n","\n","    #print(model_warm_start.conv1.weight)\n","    final_params = model_warm_start.state_dict()\n","\n","    return initial_params, final_params\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test_warm_start_model(model_warm_start):\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data\n","            outputs = model_warm_start(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    # print(f'Accuracy of the warm started network on the 10000 test images: {100 * correct / total:.2f}%')\n","\n","    return 100 * correct / total\n","\n","# correct = 0\n","# total = 0\n","\n","# with torch.no_grad():\n","#     for data in testloader:\n","#         images, labels = data\n","#         outputs = model_warm_start(images)\n","#         _, predicted = torch.max(outputs.data, 1)\n","#         total += labels.size(0)\n","#         correct += (predicted == labels).sum().item()\n","\n","# print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["randModel1, cos_models, _ = generate_models()\n","\n","\n","\n","summary(randModel1, (1, 28,28))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","cosine_similarities = []\n","cosine_rand_accuracies = []\n","cosine_warm_accuracies = []\n","cos_list_of_indexes = []\n","\n","for i in range(20):\n","\n","    rand_model, cos_models, _ = generate_models()\n","    #cos_models = cos_models[0]\n","    rand_optimizer, cos_optimizers = gen_optimizers(rand_model, cos_models)\n","    #print(cos_optimizers[0])\n","\n","\n","    train_models(cos_models, cos_optimizers)\n","    initial_params_rand, final_params_rand = train_rand_model(rand_model, rand_optimizer)\n","    similarity_scores_1, similarity_scores_2, similarity_scores_3, similarity_scores_4, similarity_scores_5, similarity_scores_6, similarity_scores_7, similarity_scores_8, similarity_scores_9, similarity_scores_10 = compare_filters(cos_models, metrics='overall_sim')\n","    indexes = get_max_index(similarity_scores_1, similarity_scores_2, similarity_scores_3, similarity_scores_4, similarity_scores_5, similarity_scores_6, similarity_scores_7, similarity_scores_8, similarity_scores_9, similarity_scores_10)\n","    cos_list_of_indexes.append(indexes)\n","    selected_filters = select_filters(cos_models, indexes)\n","    model_warm_start = gen_warm_start_model()\n","    initial_params_cos, final_params_cos = train_warm_start_model(model_warm_start)\n","    accuracy = test_warm_start_model(model_warm_start)\n","    rand_accuracy = test_model(rand_model)\n","\n","    cosine_similarities.append([similarity_scores_1, similarity_scores_2, similarity_scores_3, similarity_scores_4, similarity_scores_5, similarity_scores_6, similarity_scores_7, similarity_scores_8, similarity_scores_9, similarity_scores_10])\n","    cosine_rand_accuracies.append(rand_accuracy)\n","    cosine_warm_accuracies.append(accuracy)\n","\n","\n","\n","\n","pearson_similarities = []\n","pearson_rand_accuracies = []\n","pearson_warm_accuracies = []\n","pear_list_of_indexes = []\n","\n","for i in range(20):\n","\n","    rand_model, pear_models, _ = generate_models()\n","    #pear_models = pear_models[0]\n","    rand_optimizer, pear_optimizers = gen_optimizers(rand_model, pear_models)\n","    #print(pear_optimizers[0])\n","    \n","    train_models(pear_models, pear_optimizers)\n","    similarity_scores_1, similarity_scores_2, similarity_scores_3, similarity_scores_4, similarity_scores_5, similarity_scores_6, similarity_scores_7, similarity_scores_8, similarity_scores_9, similarity_scores_10 = compare_filters(pear_models, metrics='pearson_coeff')\n","    indexes = get_max_index(similarity_scores_1, similarity_scores_2, similarity_scores_3, similarity_scores_4, similarity_scores_5, similarity_scores_6, similarity_scores_7, similarity_scores_8, similarity_scores_9, similarity_scores_10)\n","    pear_list_of_indexes.append(indexes)\n","    selected_filters = select_filters(pear_models, indexes)\n","    model_warm_start = gen_warm_start_model()\n","    initial_params_pear, final_params_pear = train_warm_start_model(model_warm_start)\n","    accuracy = test_warm_start_model(model_warm_start)\n","    #rand_accuracy = test_model(pear_models[0])\n","\n","    pearson_similarities.append([similarity_scores_1, similarity_scores_2, similarity_scores_3, similarity_scores_4, similarity_scores_5, similarity_scores_6, similarity_scores_7, similarity_scores_8, similarity_scores_9, similarity_scores_10])\n","    #pearson_rand_accuracies.append(rand_accuracy)\n","    pearson_warm_accuracies.append(accuracy)\n","\n","\n","\n","frob_norm_similarities = []\n","frob_norm_rand_accuracies = []\n","frob_norm_warm_accuracies = []\n","frob_list_of_indexes = []\n","\n","for i in range(20):\n","\n","    rand_models, frob_models, _ = generate_models()\n","    #frob_models = frob_models[0]\n","    frob_optimizers, frob_optimizers = gen_optimizers(rand_model, frob_models)\n","    #print(frob_optimizers[0])\n","        \n","    train_models(frob_models, frob_optimizers)\n","    similarity_scores_1, similarity_scores_2, similarity_scores_3, similarity_scores_4, similarity_scores_5, similarity_scores_6, similarity_scores_7, similarity_scores_8, similarity_scores_9, similarity_scores_10 = compare_filters(frob_models, metrics='frob_norm')\n","    indexes = get_min_index(similarity_scores_1, similarity_scores_2, similarity_scores_3, similarity_scores_4, similarity_scores_5, similarity_scores_6, similarity_scores_7, similarity_scores_8, similarity_scores_9, similarity_scores_10)\n","    frob_list_of_indexes.append(indexes)\n","    selected_filters = select_filters(frob_models, indexes)\n","    model_warm_start = gen_warm_start_model()\n","    initial_params_frob, final_params_frob = train_warm_start_model(model_warm_start)\n","    accuracy = test_warm_start_model(model_warm_start)\n","    #rand_accuracy = test_model(frob_models[0])\n","\n","    frob_norm_similarities.append([similarity_scores_1, similarity_scores_2, similarity_scores_3, similarity_scores_4, similarity_scores_5, similarity_scores_6, similarity_scores_7, similarity_scores_8, similarity_scores_9, similarity_scores_10])\n","    #frob_norm_rand_accuracies.append(rand_accuracy)\n","    frob_norm_warm_accuracies.append(accuracy)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cosine_avg_acc = np.mean(cosine_warm_accuracies)\n","cosine_avg_rand_acc = np.mean(cosine_rand_accuracies)\n","pearson_avg_acc = np.mean(pearson_warm_accuracies)\n","#pearson_avg_rand_acc = np.mean(pearson_rand_accuracies)\n","frob_norm_avg_acc = np.mean(frob_norm_warm_accuracies)\n","#frob_norm_avg_rand_acc = np.mean(frob_norm_rand_accuracies)\n","\n","cosine_std_acc = np.std(cosine_warm_accuracies)\n","pearson_std_acc = np.std(pearson_warm_accuracies)\n","frob_norm_std_acc = np.std(frob_norm_warm_accuracies)\n","\n","random_init_std_acc = np.std(cosine_rand_accuracies)\n","\n","cosine_acc_max = np.max(cosine_warm_accuracies)\n","pearson_avg_max = np.max(pearson_warm_accuracies)\n","frob_norm_avg_max = np.max(frob_norm_warm_accuracies)\n","cosine_avg_max_acc = np.max(cosine_rand_accuracies)\n","\n","print(\"Cosine similarity average accuracy:\", cosine_avg_acc)\n","print(\"Average random init accuracy:\", cosine_avg_rand_acc)\n","print(\"Pearson coefficient average accuracy:\", pearson_avg_acc)\n","\n","print(\"Frobenius norm average accuracy:\", frob_norm_avg_acc)\n","\n","print(\"Random init standard deviation:\", random_init_std_acc)\n","\n","print(\"Cosine similarity standard deviation:\", cosine_std_acc)\n","print(\"Pearson coefficient standard deviation:\", pearson_std_acc)\n","print(\"Frobenius norm standard deviation:\", frob_norm_std_acc)\n","\n","\n","print(\"Max random init accuracy:\", cosine_avg_max_acc)\n","\n","print(\"Cosine similarity max accuracy:\", cosine_acc_max)\n","print(\"Pearson coefficient max accuracy:\", pearson_avg_max)\n","print(\"Frobenius norm max accuracy:\", frob_norm_avg_max)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cosine_avg_acc = np.mean(cosine_warm_accuracies)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cosine_similarities = np.array(cosine_similarities)\n","print(np.array(similarity_scores_1).shape)\n","print(cosine_similarities.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["avg_cosine_sim = np.mean(cosine_similarities, axis=(0,2))\n","avg_pearson_sim = np.mean(pearson_similarities, axis=(0,2))\n","avg_frob_norm_sim = np.mean(frob_norm_similarities, axis=(0,2))\n","\n","print(\"Cosine similarities :\", np.max(cosine_similarities))\n","print(\"Pearson similarities:\", np.max(pearson_similarities))\n","print(\"Frob norm similarities:\", np.min(frob_norm_similarities))\n","\n","print(cosine_similarities.shape)\n","frob_norm_similarities = np.array(frob_norm_similarities)\n","pearson_similarities = np.array(pearson_similarities)\n","print(\"index cos max: \", np.unravel_index(np.argmax(cosine_similarities), cosine_similarities.shape))\n","print(\"index pear max: \", np.unravel_index(np.argmax(pearson_similarities), cosine_similarities.shape))\n","print(\"index frob min:\", np.unravel_index(np.argmin(frob_norm_similarities), frob_norm_similarities.shape))\n","\n","print(\"index cos min: \", np.unravel_index(np.argmin(cosine_similarities), cosine_similarities.shape))\n","print(\"index pear min:\", np.unravel_index(np.argmin(pearson_similarities), pearson_similarities.shape))\n","print(\"index frob min:\", np.unravel_index(np.argmax(frob_norm_similarities), frob_norm_similarities.shape))\n","\n","std_cosine_sim = np.std(cosine_similarities, axis=(0,2))\n","std_pearson_sim = np.std(pearson_similarities, axis=(0,2))\n","std_frob_norm_sim = np.std(frob_norm_similarities, axis=(0,2))\n","\n","\n","print(\"Cosine similarity average:\", avg_cosine_sim)\n","print(\"Pearson coefficient average:\", avg_pearson_sim)\n","print(\"Frobenius norm average:\", avg_frob_norm_sim)\n","\n","print(\"Cosine similarity std:\", std_cosine_sim)\n","print(\"Pearson coefficient std:\", std_pearson_sim)\n","print(\"Frobenius norm std:\", std_frob_norm_sim)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy import stats\n","\n","\n","cos_list_of_indexes = np.array(cos_list_of_indexes)\n","print(cos_list_of_indexes.shape)\n","cos_list_of_indexes = cos_list_of_indexes.flatten()\n","print(cos_list_of_indexes.shape)\n","mode_cos = stats.mode(cos_list_of_indexes, axis=0)\n","print(\"mode of cosine sim :\", mode_cos)\n","\n","pear_list_of_indexes = np.array(pear_list_of_indexes)\n","pear_list_of_indexes = pear_list_of_indexes.flatten()\n","mode_pear = stats.mode(pear_list_of_indexes, axis=0)\n","print(\"mode of pear sim: \", mode_pear)\n","\n","frob_list_of_indexes = np.array(frob_list_of_indexes)\n","frob_list_of_indexes = frob_list_of_indexes.flatten()\n","mode_frob = stats.mode(frob_list_of_indexes, axis=0)\n","print(\"mode of frob sim: \", mode_frob)\n","\n","# cos most similar filter 0 models (2,3) \n","ms1 = cos_models[1]\n","ms2 = cos_models[2]\n","\n","ms_filter1 = ms1.conv1.weight[0]\n","print(ms_filter1.shape)\n","ms_filter1 = ms_filter1.detach().numpy()\n","ms_filter1 = ms_filter1.reshape((3,3))\n","\n","ms_filter2 = ms2.conv1.weight[0]\n","ms_filter2 = ms_filter2.detach().numpy()\n","ms_filter2 = ms_filter2.reshape((3,3))\n","\n","\n","\n","print(ms_filter)\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Create a figure with two subplots\n","fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n","\n","# Plot the first heatmap\n","sns.heatmap(ms_filter1, annot=False, cmap='plasma', fmt='g', ax=axes[0])\n","\n","\n","\n","# Plot the second heatmap\n","sns.heatmap(ms_filter2, annot=False, cmap='plasma', fmt='g', ax=axes[1])\n","\n","\n","fig.suptitle('Heatmap of Most Similar Filters with Cosine Metric', fontsize=16)\n","# Adjust the layout to prevent overlap\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","\n","# Show the plot\n","plt.show()\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# cos least similar (1,4) filter 2\n","ms1 = cos_models[0]\n","ms2 = cos_models[3]\n","\n","ms_filter1 = ms1.conv2.weight[0][0]\n","print(ms_filter1.shape)\n","ms_filter1 = ms_filter1.detach().numpy()\n","ms_filter1 = ms_filter1.reshape((3,3))\n","\n","ms_filter2 = ms2.conv2.weight[0][0]\n","ms_filter2 = ms_filter2.detach().numpy()\n","ms_filter2 = ms_filter2.reshape((3,3))\n","\n","\n","\n","print(ms_filter)\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","# Create a figure with two subplots\n","fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n","\n","# Plot the first heatmap\n","sns.heatmap(ms_filter1, annot=False, cmap='plasma', fmt='g', ax=axes[0])\n","\n","\n","\n","# Plot the second heatmap\n","sns.heatmap(ms_filter2, annot=False, cmap='plasma', fmt='g', ax=axes[1])\n","\n","\n","fig.suptitle('Heatmap of Least Similar Filters with Cosine Metric', fontsize=16)\n","# Adjust the layout to prevent overlap\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# pear most similar filter 1 (3,5)\n","#(1,2) (1,3) (1,4) (1,5) (1,6) (2,3) (2,4) (2,5) (2,6)  (3,4) (3,5) (3,6) (4,5) (4,6) (5,6)\n","ms1 = pear_models[2]\n","ms2 = pear_models[4]\n","\n","ms_filter1 = ms1.conv1.weight[1]\n","print(ms_filter1.shape)\n","ms_filter1 = ms_filter1.detach().numpy()\n","ms_filter1 = ms_filter1.reshape((3,3))\n","\n","ms_filter2 = ms2.conv1.weight[1]\n","ms_filter2 = ms_filter2.detach().numpy()\n","ms_filter2 = ms_filter2.reshape((3,3))\n","\n","\n","\n","print(ms_filter)\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Create a figure with two subplots\n","fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n","\n","# Plot the first heatmap\n","sns.heatmap(ms_filter1, annot=False, cmap='plasma', fmt='g', ax=axes[0])\n","\n","\n","\n","# Plot the second heatmap\n","sns.heatmap(ms_filter2, annot=False, cmap='plasma', fmt='g', ax=axes[1])\n","\n","\n","fig.suptitle('Heatmap of Most Similar Filters with Pearson Correlation', fontsize=16)\n","# Adjust the layout to prevent overlap\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# pear least similar filter 6 (4,6)\n","#(1,2) (1,3) (1,4) (1,5) (1,6) (2,3) (2,4) (2,5) (2,6)  (3,4) (3,5) (3,6) (4,5) (4,6) (5,6)\n","ms1 = cos_models[3]\n","ms2 = cos_models[5]\n","\n","ms_filter1 = ms1.conv2.weight[1][1]\n","print(ms_filter1.shape)\n","ms_filter1 = ms_filter1.detach().numpy()\n","ms_filter1 = ms_filter1.reshape((3,3))\n","\n","ms_filter2 = ms2.conv2.weight[1][1]\n","ms_filter2 = ms_filter2.detach().numpy()\n","ms_filter2 = ms_filter2.reshape((3,3))\n","\n","\n","\n","print(ms_filter)\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","# Create a figure with two subplots\n","fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n","\n","# Plot the first heatmap\n","sns.heatmap(ms_filter1, annot=False, cmap='plasma', fmt='g', ax=axes[0])\n","\n","\n","\n","# Plot the second heatmap\n","sns.heatmap(ms_filter2, annot=False, cmap='plasma', fmt='g', ax=axes[1])\n","\n","\n","fig.suptitle('Heatmap of Least Similar Filters with Pearson Correlation', fontsize=16)\n","# Adjust the layout to prevent overlap\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# frob most similar filter 9 (3,6)\n","#(1,2) (1,3) (1,4) (1,5) (1,6) (2,3) (2,4) (2,5) (2,6)  (3,4) (3,5) (3,6) (4,5) (4,6) (5,6)\n","ms1 = cos_models[2]\n","ms2 = cos_models[5]\n","\n","ms_filter1 = ms1.conv2.weight[3][0]\n","print(ms_filter1.shape)\n","ms_filter1 = ms_filter1.detach().numpy()\n","ms_filter1 = ms_filter1.reshape((3,3))\n","\n","ms_filter2 = ms2.conv2.weight[3][0]\n","ms_filter2 = ms_filter2.detach().numpy()\n","ms_filter2 = ms_filter2.reshape((3,3))\n","\n","\n","\n","print(ms_filter)\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","# Create a figure with two subplots\n","fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n","\n","# Plot the first heatmap\n","sns.heatmap(ms_filter1, annot=False, cmap='plasma', fmt='g', ax=axes[0])\n","\n","\n","\n","# Plot the second heatmap\n","sns.heatmap(ms_filter2, annot=False, cmap='plasma', fmt='g', ax=axes[1])\n","\n","\n","fig.suptitle('Heatmap of Most Similar Filters with Frobenius Norm', fontsize=16)\n","# Adjust the layout to prevent overlap\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# pear least similar filter 1 (3,4)\n","#(1,2) (1,3) (1,4) (1,5) (1,6) (2,3) (2,4) (2,5) (2,6)  (3,4) (3,5) (3,6) (4,5) (4,6) (5,6)\n","ms1 = cos_models[2]\n","ms2 = cos_models[3]\n","\n","ms_filter1 = ms1.conv1.weight[1]\n","print(ms_filter1.shape)\n","ms_filter1 = ms_filter1.detach().numpy()\n","ms_filter1 = ms_filter1.reshape((3,3))\n","\n","ms_filter2 = ms2.conv1.weight[1]\n","ms_filter2 = ms_filter2.detach().numpy()\n","ms_filter2 = ms_filter2.reshape((3,3))\n","\n","\n","\n","print(ms_filter)\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Create a figure with two subplots\n","fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n","\n","# Plot the first heatmap\n","sns.heatmap(ms_filter1, annot=False, cmap='plasma', fmt='g', ax=axes[0])\n","\n","\n","\n","# Plot the second heatmap\n","sns.heatmap(ms_filter2, annot=False, cmap='plasma', fmt='g', ax=axes[1])\n","\n","\n","fig.suptitle('Heatmap of Least Similar Filters with Frobenius Norm', fontsize=16)\n","# Adjust the layout to prevent overlap\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def calc_sim_init_final_weights(initial_params, final_params):\n","    print(initial_params.keys())\n","    initial_filters = [initial_params['conv1.weight'][0], initial_params['conv1.weight'][1], initial_params['conv2.weight'][0][0], initial_params['conv2.weight'][0][1], initial_params['conv2.weight'][1][0], initial_params['conv2.weight'][1][1], initial_params['conv2.weight'][2][0], initial_params['conv2.weight'][2][1], initial_params['conv2.weight'][3][0], initial_params['conv2.weight'][3][1]]\n","    final_filters = [final_params['conv1.weight'][0], final_params['conv1.weight'][1], final_params['conv2.weight'][0][0], final_params['conv2.weight'][0][1], final_params['conv2.weight'][1][0], final_params['conv2.weight'][1][1], final_params['conv2.weight'][2][0], final_params['conv2.weight'][2][1], final_params['conv2.weight'][3][0], final_params['conv2.weight'][3][1]]\n","    sims = []\n","    sims_rand = []\n","\n","    #final_rand_params = model.state_dict()\n","    #initial_rand_filters = [initial_rand_parms['conv1.weight'][0], initial_rand_parms['conv1.weight'][1], initial_rand_parms['conv2.weight'][0][0], initial_rand_parms['conv2.weight'][0][1], initial_rand_parms['conv2.weight'][1][0], initial_rand_parms['conv2.weight'][1][1], initial_rand_parms['conv2.weight'][2][0], initial_rand_parms['conv2.weight'][2][1], initial_rand_parms['conv2.weight'][3][0], initial_rand_parms['conv2.weight'][3][1]]\n","    #final_rand_filters = [final_rand_params['conv1.weight'][0], final_rand_params['conv1.weight'][1], final_rand_params['conv2.weight'][0][0], final_rand_params['conv2.weight'][0][1], final_rand_params['conv2.weight'][1][0], final_rand_params['conv2.weight'][1][1], final_rand_params['conv2.weight'][2][0], final_rand_params['conv2.weight'][2][1], final_rand_params['conv2.weight'][3][0], final_rand_params['conv2.weight'][3][1]]\n","\n","\n","    for i in range(10):\n","        #print(initial_filters[i].shape)\n","        filter1 = initial_filters[i].reshape(3,3)\n","        #print(filter1.shape)\n","        filter2 = final_filters[i].reshape(3,3)\n","        similarity = calc_pearson_coeff(filter1, filter2)\n","        sims.append(similarity)\n","\n","    #print(sims)\n","\n","    return sims\n","\n","\n","    # for i in range(10):\n","        \n","    #     filter1 = initial_rand_filters[i].reshape(3,3)\n","        \n","    #     filter2 = final_rand_filters[i].reshape(3,3)\n","    #     similarity = calc_pearson_coeff(filter1, filter2)\n","    #     sims_rand.append(similarity)\n","\n","    # print(sims_rand)\n","\n","sims_rand = calc_sim_init_final_weights(initial_params_rand, final_params_rand)\n","sims_cos = calc_sim_init_final_weights(initial_params_cos, final_params_cos)\n","sims_pear = calc_sim_init_final_weights(initial_params_pear, final_params_pear)\n","sims_frob = calc_sim_init_final_weights(initial_params_frob, final_params_frob)\n","\n","\n","print(\"Random initialisation similarity between initial and final filters: \", sims_rand)\n","print(\"Cosine similarity warm started model similarity betweeen initial (chosen) and final weights\", sims_cos)\n","print(\"Pearson similarity warm started model similarity between initial (chosen) and final weights\", sims_pear)\n","print(\"Frob similarity warm started model similarity between initial (chosen) and final weights\", sims_frob)\n","\n","avg_sims_rand = np.mean(sims_rand)\n","avg_sims_cos = np.mean(sims_cos)\n","avg_sims_pear = np.mean(sims_pear)\n","avg_sims_frob = np.mean(sims_frob)\n","\n","\n","print(\"Avg rand\", avg_sims_rand)\n","print(\"Avg cos\", avg_sims_cos)\n","print(\"Avg pear\", avg_sims_pear)\n","print(\"Avg frob\", avg_sims_frob)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define the training loop function\n","def train_model(model, optimizer, criterion, train_loader, test_loader, max_epochs=100):\n","    train_losses = []\n","    test_losses = []\n","    train_accs = []\n","    test_accs = []\n","    \n","    best_test_loss = float('inf')\n","    best_model_weights = None\n","    \n","    for epoch in range(max_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        correct = 0\n","        total = 0\n","        \n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            \n","            train_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","        \n","        train_loss /= len(train_loader)\n","        train_acc = 100 * correct / total\n","        train_losses.append(train_loss)\n","        train_accs.append(train_acc)\n","        \n","        model.eval()\n","        test_loss = 0.0\n","        correct = 0\n","        total = 0\n","        \n","        with torch.no_grad():\n","            for inputs, labels in test_loader:\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                \n","                test_loss += loss.item()\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","        \n","        test_loss /= len(test_loader)\n","        test_acc = 100 * correct / total\n","        test_losses.append(test_loss)\n","        test_accs.append(test_acc)\n","        \n","        if test_loss < best_test_loss:\n","            best_test_loss = test_loss\n","            best_model_weights = model.state_dict()\n","\n","    return train_losses, test_losses, train_accs, test_accs, best_model_weights\n","\n","\n","\n","\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
